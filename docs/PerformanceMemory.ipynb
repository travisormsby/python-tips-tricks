{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use Python for automation, and if we're expecting to use the same code--or variations of it--over and over again, then it's worthwhile to figure out what's slowing it down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Access and store only what you need, no more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl;dr \n",
    "Look out for unnecessary memory use.\n",
    "- Avoid lists where you could use tuples\n",
    "- Avoid lists for checking membership where you could use sets\n",
    "- Avoid sequences where you could use iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### lists vs. tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you have a collection of values, your first thought may be to store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_list = [17999712, 2015, 'Hawkins Road', 'Linden ', 'NC', 28356]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lists are nice because they are very flexible. You can change the values in the list, including appending and removing values. But that flexibility comes at a cost. Lists are less efficient than tuples. For example, they use more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "data_tuple = (17999712, 2015, 'Hawkins Road', 'Linden ', 'NC', 28356)\n",
    "\n",
    "print(sys.getsizeof(data_list))\n",
    "print(sys.getsizeof(data_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you aren't going to be changing the values in a collection, use a tuple instead of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### lists vs. sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lists perform faster for ordered operations (think: sorting). But sets and dictionaries are better for lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you want to see if an element already exists in a collection of elements, use a set to store that collection if possible. Lists and tuples require sequential lookup to see if an element is a member of the collection. That means that on average, it has to make n/2 comparisons for a collection of length n. Because every element in a set is unique, however, a set can use a hash table for lookups. That means no matter how big the collection is, the set only ever has to check 1 value. The bigger the collection is, the bigger the gap between sets and lists becomes. \n",
    "The hash is what makes sets (and dicts) so fast for lookups. Instead of searching every item, Python jumps directly to the hash bucket. When you add an element to a set, 1) Python calls the element’s __hash__() method to get a hash value (an integer); 2) That hash value determines where the element will be stored in the set's internal structure; 3) When checking if an element is in the set, Python uses the hash to quickly find it. \n",
    "\n",
    "The example below shows that a set is over 1000x faster than a list in calculating the first 100,000 values of [Recaman's sequence](https://oeis.org/search?q=recaman&language=english&go=Search) but the concept applies any time you are checking for membership in a large collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recaman_check(cur, i, visited):\n",
    "    return (cur - i) < 0 or (cur - i) in visited\n",
    "\n",
    "def recaman_list(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    return a list of the first n numbers of the Recaman series\n",
    "    \"\"\"\n",
    "    \n",
    "    visited_list = [0]\n",
    "    current = 0\n",
    "    for i in range(1, n):\n",
    "        if recaman_check(current, i, visited_list):\n",
    "            current += i\n",
    "        else:\n",
    "            current -= i\n",
    "        visited_list.append(current)\n",
    "    return visited_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2 s ± 20.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "recaman_list(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recaman_set(n: int) -> list[int]:\n",
    "    visited_set = {0}\n",
    "    current = 0\n",
    "    for i in range(1, 100_000):\n",
    "        if recaman_check(current, i, visited_set):\n",
    "            current += i\n",
    "        else:\n",
    "            current -= i\n",
    "        visited_set.add(current)\n",
    "    return visited_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "recaman_set(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### sequences vs. iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Regular functions and comprehensions typically create a container type like a list or a dictionary to store the data that results from the function’s intended computation. All this data is stored in memory at the same time.\n",
    "\n",
    "In contrast, iterators keep only one data item in memory at a time, generating the next items on demand or lazily. \n",
    "\n",
    "With iterators and generators, you don’t need to store all the data in your compter’s memory at the same time.\n",
    "\n",
    "Iterators and generators also allow you to completely decouple iteration from processing individual items. They let you connect multiple data processing stages to create memory-efficient data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.linkedin.com/pulse/python-generators-understanding-power-different-use-cases-singh/\n",
    "https://nedbatchelder.com/text/iter.html\n",
    "https://github.com/pythological/kanren\n",
    "https://discuss.python.org/t/the-pros-cons-of-generators/30950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance: Profile your code for bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl;dr Make time for performance checks anytime your code takes longer than ___ . There are several resources:\n",
    "1. %%timeit is useful when...\n",
    "Use the `%%timeit` notebook magic to perform some basic profiling. We know that the first cell is much slower than the second. But `%%timeit` isn't precise enough to tell which calls in each cell are taking the longest to execute.\n",
    "2. cProfile is useful when...\n",
    "The `cProfile` module has the ability to break down call by call to determine the number of calls and the total time spent on each. \n",
    "Look at tottime to find which functions are slow themselves. Look at cumtime to find functions that might call other slow functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Workflow for Performance and Memory\n",
    "1. Spot-check for instances of unnecessary memory use\n",
    "1. Replace above instances with low-memory alternatives\n",
    "1. If necessary, create a sample to profile on: same complexity, smaller size. Then:\n",
    "1. Profile: Check for speed bottlenecks at a high level (%%timeit) \n",
    "1. Profile: For the slowest cell from prev step: check for speed bottlenecks at a granular level (cProfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise steps:__\n",
    "1. Replace lists with memory-saving alternatives\n",
    "    1. Replace with tuple\n",
    "    1. Replace with set\n",
    "    1. Replace with set (2nd example)\n",
    "    1. Compare differences in speed with `%%timeit`\n",
    "1. Replace sequences with memory-saving alternatives\n",
    "    1. Convert list comprehension to generator expression\n",
    "    1. Convert loop to intersection\n",
    "    1. Compare differences in speed with `%%timeit`\n",
    "1. Check for speed bottlenecks in detail with `cProfile`\n",
    "    1. Apply for all the above code blocks\n",
    "    1. Inspect results\n",
    "1. Challenge: Improve readability across the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Prepare workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Replace lists with memory-saving alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: You have CSVs stored in a subdirectory. Some of these CSVs can be converted to point shapefiles. For every CSV in the folder, you need to determine whether it contains a field called 'lat', which would indicate it has point coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of file paths where the data is stored.\n",
    "myDataPaths = [filePath for file in directory]\n",
    "\n",
    "\n",
    "# Define a function for determining if a file meets your criteria.\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if criterium in headerList:\n",
    "                members.append(filePath)\n",
    "    return members\n",
    "\n",
    "\n",
    "# Print all matching file paths\n",
    "print(meetsCriteria(myDataPaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change a list to a tuple to reduce its storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the filePaths list to a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solution\n",
    "myDataPaths = (filePath for file in directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify membership using set intersection instead of list look-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *meetsCriteria()* function, search for each CSV's fieldnames using set intersection.\n",
    "<br><br>Actions:\n",
    "- Change the if statement to use *.intersection()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solution\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if headerList.intersection(criterium) is not None:\n",
    "                members.append(filePath)\n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change list to a set to reduce its storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the meetsCriteria function to add files to a set instead of appending to a list.\n",
    "<br><br>Actions:\n",
    "- Change the variable *members* to a set.\n",
    "- Modify the line *members.append(filePath)* to use the *.add()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solution\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = {}\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if headerList.intersection(criterium) is not None:\n",
    "                members.add(filePath)\n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare differences in speed using `%%timeit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `%%timeit`, compare the time it took to create myDataPaths as a list (original code) versus as a tuple (exercise solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "print([filePath for file in directory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "## Your solution here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `%%timeit` again to compare list-based lookup to set intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if criterium in headerList:\n",
    "                members.append(filePath)\n",
    "    return members\n",
    "\n",
    "\n",
    "# Print all matching file paths\n",
    "print(meetsCriteria(myDataPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "## Your solution here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compare the second list vs. set change that you made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if criterium in headerList:\n",
    "                members.append(filePath)\n",
    "    return members\n",
    "\n",
    "\n",
    "# Print all matching file paths\n",
    "print(meetsCriteria(myDataPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "## Your solution here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Replace sequences with memory-saving alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Check for speed bottlenecks in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cProfile to locate the slowest calls in the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cProfile to locate the slowest calls in your improved script. \n",
    "Hint: Sort by tottime instead of name to find hotspots more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution from the previous steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of file paths where the data is stored.\n",
    "myDataPaths = (filePath for file in directory)\n",
    "\n",
    "\n",
    "# Define a function for determining if a file meets your criteria.\n",
    "def meetsCriteria(filePaths):\n",
    "    \"\"\"\n",
    "    Dataframe must have a 'lat' field to be included.\n",
    "    \"\"\"\n",
    "    members = {}\n",
    "    criterium = 'lat'\n",
    "    \n",
    "    for filePath in filePaths:\n",
    "        with open(filePath) as fPath:\n",
    "            headerList = csv.DictReader(fPath).fieldnames\n",
    "            if headerList.intersection(criterium) is not None:\n",
    "                members.add(filePath)\n",
    "    return members\n",
    "\n",
    "\n",
    "# Print all matching file paths\n",
    "print(meetsCriteria(myDataPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Challenge: Improve readability across the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function *meetsCriteria()*, add more detail in the doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a database (geopackage) of a city's building footprint geometries. Each layer represents the new construction built in that year. What you need instead, however, is a snapshot of total built area each year: all of that year's construction plus anything existing from previous years in the study.\n",
    "\n",
    "To create this new database, for every year in the study, you will subset building features for that year and all previous years, dissolve them to create one cumulative feature, and then save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftPrints = gpd.read_file(\"City.gpkg\", layer=\"Footprints\")\n",
    "constructYears = [1999, 2000, 2001, 2002, 2003, 2004]\n",
    "startYear = 1999\n",
    "print('Prepared to dissolve building footprints for cumulative years from 1999 to 2004.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEFORE: for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to perform this workflow is with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in constructYears:\n",
    "    timeframe = ftPrints[ftPrints['year'].between(startYear, year, inclusive=True)] \n",
    "    tfDissolve = timeframe.dissolve(by='ID',\n",
    "                                      aggfunc={\"year\": \"max\"},\n",
    "                                      as_index=False)\n",
    "    \n",
    "    yearName = ''.join(['cu', str(year)])\n",
    "    tfDissolve.to_file(driver='GPKG', filename='cuFootprints.gpkg', layer=yearName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AFTER: generator\n",
    "*(exercise solution)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve this code by creating a data pipeline of generator expressions, ending in a for loop for only the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generator\n",
    "timeframes = (ftPrints[ftPrints['year'].between(startYear, year, inclusive=True)] for year in constructYears)\n",
    "\n",
    "# Second generator\n",
    "tfDissolve = (timeframe.dissolve(by='ID', aggfunc={\"year\": \"max\"}, as_index=False) for timeframe in timeframes)\n",
    "\n",
    "# Third generator\n",
    "yearName = (''.join(['cu', str(year)]) for year in constructYears)\n",
    "\n",
    "# Commence iteration\n",
    "for year in constructYears:\n",
    "    tfDissolve.to_file(driver='GPKG', filename='cuFootprints.gpkg', layer=yearName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Profile your code instead of guessing about performance bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recaman_check(cur, i, visited):\n",
    "    return (cur - i) < 0 or (cur - i) in visited\n",
    "\n",
    "def recaman_list(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    return a list of the first n numbers of the Recaman series\n",
    "    \"\"\"\n",
    "    \n",
    "    visited_list = [0]\n",
    "    current = 0\n",
    "    for i in range(1, n):\n",
    "        if recaman_check(current, i, visited_list):\n",
    "            current += i\n",
    "        else:\n",
    "            current -= i\n",
    "        visited_list.append(current)\n",
    "    return visited_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2 s ± 20.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "recaman_list(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recaman_set(n: int) -> list[int]:\n",
    "    visited_set = {0}\n",
    "    current = 0\n",
    "    for i in range(1, 100_000):\n",
    "        if recaman_check(current, i, visited_set):\n",
    "            current += i\n",
    "        else:\n",
    "            current -= i\n",
    "        visited_set.add(current)\n",
    "    return visited_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "recaman_set(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above uses the `%%timeit` notebook magic to perform some basic profiling. We know that the first cell is much slower than the second. But `%%timeit` isn't precise enough to tell which calls in each cell are taking the longest to execute. Are sets faster because it's easier to check if an element is in a set? Or are sets faster because it is easier to add an element to a set than append an element to a list?\n",
    "\n",
    "The `cProfile` module has the ability to break down call by call to determine the number of calls and the total time spent on each. Running each function with cProfile demonstrates that the in the list version of the function, the time to append elements is dwarfed by the time it takes to check if an element is in a list. The set version of the function is actually a little slower to add elements, but it is much faster to check elements for membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         200002 function calls in 19.888 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    99999   19.808    0.000   19.808    0.000 1555087545.py:1(recaman_check)\n",
      "        1    0.072    0.072   19.888   19.888 1555087545.py:4(recaman_list)\n",
      "        1    0.001    0.001   19.888   19.888 <string>:1(<module>)\n",
      "        1    0.000    0.000   19.888   19.888 {built-in method builtins.exec}\n",
      "    99999    0.007    0.000    0.007    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run('recaman_list(100000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         200002 function calls in 0.068 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    99999    0.018    0.000    0.018    0.000 1555087545.py:1(recaman_check)\n",
      "        1    0.041    0.041    0.068    0.068 662776923.py:1(recaman_set)\n",
      "        1    0.001    0.001    0.068    0.068 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.068    0.068 {built-in method builtins.exec}\n",
      "    99999    0.009    0.000    0.009    0.000 {method 'add' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('recaman_set(100000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use more tuples and fewer lists\n",
    "If you have a collection of values, your first thought may be to store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [17999712, 2015, 'Hawkins Road', 'Linden ', 'NC', 28356]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists are nice because they are very flexible. You can change the values in the list, including appending and removing values. But that flexibility comes at a cost. Lists are less efficient than tuples. For example, they use more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "data_tuple = (17999712, 2015, 'Hawkins Road', 'Linden ', 'NC', 28356)\n",
    "\n",
    "print(sys.getsizeof(data_list))\n",
    "print(sys.getsizeof(data_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't going to be changing the values in a collection, use a tuple instead of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Use more iterators and fewer lists/tuples for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of whether you use a list or a tuple, you need to store a reference to every value in memory. Both lists and tuples take up more memory the bigger they get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "80040\n"
     ]
    }
   ],
   "source": [
    "little_tuple = tuple(range(10))\n",
    "big_tuple = tuple(range(10000))\n",
    "\n",
    "print(sys.getsizeof(little_tuple))\n",
    "print(sys.getsizeof(big_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists and tuples are iterables because you can iterate over their values. Iterators are a special type of iterable that evaluate their values lazily, only when the values are needed. They don't hold references to all the values in memory. That means iterators have constant size in memory. For example, `enumerate` produces an iterator. ArcPy cursors also produce iterators (not shown here so that this notebook works in an standard Notebook Server environment without access to ArcPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Iterator\n",
    "\n",
    "little_tuple_iterator = enumerate(little_tuple)\n",
    "big_tuple_iterator = enumerate(big_tuple)\n",
    "\n",
    "print(isinstance(little_tuple_iterator, Iterator))\n",
    "print(sys.getsizeof(little_tuple_iterator))\n",
    "print(sys.getsizeof(big_tuple_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iterator is any object that implements both the `__iter__` and `__next__` methods. You can make your own iterators by creating a custom class that implements those methods (or a class that inherits from `collections.abc.Iterator`). An easier way to create your own iterators is to create a special type of iterator called a generator. Generators are functions that use a `yield` statement to produce values lazily, one at a time (instead of all at once like a list or tuple). For example, you could use a generator to produce infinite sequential ObjectID numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is oid a generator? True\n",
      "Is oid an iterator? True\n",
      "At first the size of oid is 192 bytes\n",
      "The first value produced by oid is 1000\n",
      "After 10,000 iterations, the next value produced by oid is 11001\n",
      "After 10,000 iterations, the size of oid is 192 bytes\n"
     ]
    }
   ],
   "source": [
    "from types import GeneratorType\n",
    "\n",
    "def make_oids(start):\n",
    "    while True:\n",
    "        yield start\n",
    "        start += 1\n",
    "\n",
    "oid = make_oids(start=1000)\n",
    "\n",
    "is_generator = isinstance(oid, GeneratorType)\n",
    "is_iterator = isinstance(oid, Iterator)\n",
    "print(f\"Is oid a generator? {is_generator}\")\n",
    "print(f\"Is oid an iterator? {is_iterator}\")\n",
    "\n",
    "# Every time you want a new oid, just call next(oid). The size of oid never changes\n",
    "print(f\"At first the size of oid is {sys.getsizeof(oid)} bytes\")\n",
    "print(f\"The first value produced by oid is {next(oid)}\")\n",
    "\n",
    "for i in range(10000):\n",
    "    new_oid = next(oid)\n",
    "\n",
    "print(f\"After 10,000 iterations, the next value produced by oid is {next(oid)}\")\n",
    "print(f\"After 10,000 iterations, the size of oid is {sys.getsizeof(oid)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators (inlcuding generators) are useful for enabling better separation of concerns. Maybe you need to create many records and assign each of them an ObjectID. You could do that in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'OID': 1000, 'value': 0},\n",
       " {'OID': 1001, 'value': 1},\n",
       " {'OID': 1002, 'value': 2},\n",
       " {'OID': 1003, 'value': 3},\n",
       " {'OID': 1004, 'value': 4},\n",
       " {'OID': 1005, 'value': 5},\n",
       " {'OID': 1006, 'value': 6},\n",
       " {'OID': 1007, 'value': 7},\n",
       " {'OID': 1008, 'value': 8},\n",
       " {'OID': 1009, 'value': 9}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "oid = 1000\n",
    "for value in range(10):\n",
    "    record = {\n",
    "        'OID':  oid,\n",
    "        'value': value,\n",
    "    }\n",
    "    oid += 1\n",
    "    records.append(record)\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works, but the loop is doing two things: making a record and making an OID value. It would be better if those things were separated. That way if we later need to change the way OID values are generated, we don't have to change the code in the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'OID': 1000, 'value': 0},\n",
       " {'OID': 1001, 'value': 1},\n",
       " {'OID': 1002, 'value': 2},\n",
       " {'OID': 1003, 'value': 3},\n",
       " {'OID': 1004, 'value': 4},\n",
       " {'OID': 1005, 'value': 5},\n",
       " {'OID': 1006, 'value': 6},\n",
       " {'OID': 1007, 'value': 7},\n",
       " {'OID': 1008, 'value': 8},\n",
       " {'OID': 1009, 'value': 9}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "oid = make_oids(1000)\n",
    "for value in range(10):\n",
    "    record = {\n",
    "        'OID':  next(oid),\n",
    "        'value': value,\n",
    "    }\n",
    "    records.append(record)\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
